{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNSXeaeLkaAr"
      },
      "source": [
        "# EEC 174AY Lab B2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-8NOgK7kiqX",
        "outputId": "32fb66cc-4d0b-476a-e4e8-4128b57cdb4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "W9zadgdfkaAw",
        "outputId": "fca35b46-d8c9-43ca-a43e-396f4383f3c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              ".text_cell_render p{\n",
              "    font-size: 130%;\n",
              "    line-height: 125%;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from IPython.core.display import HTML\n",
        "HTML(\"\"\"\n",
        "<style>\n",
        ".text_cell_render p{\n",
        "    font-size: 130%;\n",
        "    line-height: 125%;\n",
        "}\n",
        "</style>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sG0nL9QkaAz"
      },
      "source": [
        "## Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGTh8yjCkaAz"
      },
      "source": [
        "This lab will build your skills in utilizing LSTM networks so that you can apply deep learning to time series information\n",
        "\n",
        "1. you will code an LSTM network and apply it to a pre-built codebase. Your focus will be on the ML coding\n",
        "2. You will utilize a partially built code base and then finish it to detect ARDS in waveform data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOuXKvnbkaA0"
      },
      "source": [
        "## LSTM Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkA4ux1ekaA1"
      },
      "source": [
        "LSTM is a network that is able to utilize time series information and learn long term patterns to make more accurate predictions than a normal neural network would be able to. We show the general network architecture as an instruction for what you will need to code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDJqPW_kaA1"
      },
      "source": [
        "# <img src=\"/content/drive/MyDrive/Colab Notebooks/Lab_B2/The_LSTM_cell.png\" width=55% height=auto\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_-8RB9kaA2"
      },
      "source": [
        "You will be applying LSTM to the task of patient ventilator asynchrony (PVA) detection. We have supplied a bit of the code you will need. Your jobs will be the following:\n",
        "\n",
        "1. Code the `find_scaling_coefs`, `scale_breath`, and `pad_or_cut_breath` methods in the `PVADataset` class in `dataset.py`.\n",
        "2. Code a simple 1 layer LSTM network based on network schematics given above. You are welcome to use other resource for assistance as well.\n",
        "3. Run your LSTM model on PVA detection. How well does your model perform compared to your original Random Forest classifier? Why are you getting these results?\n",
        "4. Code a more complex 3 layer LSTM network. Do additional layers improve results? Why/Why not?\n",
        "\n",
        "For the math required we would advise you follow the [PyTorch LSTM mathematics](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urukombNkaA2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        # Feel free to modify this\n",
        "        lstm_hidden_units = 32\n",
        "        # First layer is defined for you. Only have 2 input features (flow, pressure)\n",
        "        self.ii = nn.Linear(2, lstm_hidden_units)\n",
        "        # XXX TODO\n",
        "        self.dummy = nn.Linear(2, lstm_hidden_units)\n",
        "        self.s2 = nn.Linear(2, lstm_hidden_units)\n",
        "        self.s3 = nn.Linear(2, lstm_hidden_units)\n",
        "        self.t1 = nn.Linear(2, lstm_hidden_units)\n",
        "        # Final layer is defined for you too. Have 3 potential output classes (normal, bsa, dta)\n",
        "        self.final_classification = nn.Linear(lstm_hidden_units, 3)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # XXX code this up\n",
        "        trial = self.dummy(x[:, 1, :])\n",
        "        trial = self.sigmoid(trial)\n",
        "        ct_next = torch.zeros_like(trial)\n",
        "        for i in range(x.size(1)):\n",
        "          r = x[:, i, :]\n",
        "          ct_prev = ct_next\n",
        "          sig1 = self.ii(r)\n",
        "          sig1 = self.sigmoid(sig1)\n",
        "          sig2 = self.s2(r)\n",
        "          sig2 = self.sigmoid(sig2)\n",
        "          sig3 = self.s3(r)\n",
        "          sig3 = self.sigmoid(sig3)\n",
        "          tan1 = self.t1(r)\n",
        "          tan1 = self.tanh(tan1)\n",
        "          ct_s1 = ct_prev * sig1\n",
        "          s2_t1 = sig2 * tan1\n",
        "          ct_next = ct_s1 + s2_t1\n",
        "          ct_tan = self.tanh(ct_next)\n",
        "          out = sig3 * ct_tan\n",
        "          out = self.final_classification(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix_uyXtbkaA3",
        "outputId": "92aa11b2-9d17-4991-e876-e0342427a8ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: 0.5999, Training Accuracy: 2.0000\n",
            "Validation Results\n",
            "F1 Score: 0.0020\n",
            "Validation Loss: 0.5794, Validation Accuracy: 2.0003\n",
            "Epoch 2/5\n",
            "Training Loss: 0.5254, Training Accuracy: 2.1912\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5620, Validation Accuracy: 2.1400\n",
            "Epoch 3/5\n",
            "Training Loss: 0.5076, Training Accuracy: 2.3090\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5603, Validation Accuracy: 2.1400\n",
            "Epoch 4/5\n",
            "Training Loss: 0.5019, Training Accuracy: 2.3090\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5609, Validation Accuracy: 2.1400\n",
            "Epoch 5/5\n",
            "Training Loss: 0.4998, Training Accuracy: 2.3090\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5619, Validation Accuracy: 2.1400\n",
            "Testing Results\n",
            "F1 Score: 0.4960\n",
            "Test Accuracy: 1.9920\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from dataset import PVADataset\n",
        "\n",
        "model = LSTMNetwork().cuda()\n",
        "# You should modify the learning rate as suits the problem\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "def get_dataset(path, name):\n",
        "    saved_set = Path(path)\n",
        "    # Make sure we save previously processed data. This speeds up future processes.\n",
        "    if saved_set.exists():\n",
        "        dataset = pd.read_pickle(saved_set.resolve())\n",
        "    else:\n",
        "        # use a sequence length of 224 inputs. If you want to shorten this feel free.\n",
        "        dataset = PVADataset(name, 224)\n",
        "        dataset.process_dataset()\n",
        "        pd.to_pickle(dataset, saved_set.resolve())\n",
        "    return dataset\n",
        "\n",
        "def get_all_datasets():\n",
        "    training_set = get_dataset('pva_training_set.pkl', 'train')\n",
        "    validation_set = get_dataset('pva_validation_set.pkl', 'val')\n",
        "    testing_set = get_dataset('pva_testing_set.pkl', 'test')\n",
        "    return training_set, validation_set, testing_set\n",
        "\n",
        "def perform_training_epoch(train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_corrects = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.float().cuda()\n",
        "        y = y.float().cuda()\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        output = model(x)\n",
        "        loss = bce(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        binary_predictions = (output > 0.5).float()\n",
        "        train_corrects += torch.sum(binary_predictions == y.data)\n",
        "        train_total += y.size(0)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = train_corrects / train_total\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "def perform_inferencing(loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.float().cuda()\n",
        "\n",
        "            output = model(x)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = bce(output, y.float().cuda())\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "            # Store predictions and ground truth labels\n",
        "            all_predictions.append(output.cpu().numpy())\n",
        "            all_targets.append(y.cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            binary_predictions = (output > 0.5).cpu().float()  # Move to CPU\n",
        "            total_corrects += torch.sum(binary_predictions == y.data.cpu())\n",
        "            total_samples += y.size(0)\n",
        "\n",
        "    # Concatenate predictions and ground truth labels\n",
        "    predictions = np.concatenate(all_predictions)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(targets, binary_predictions, average='micro')\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = total_corrects / total_samples\n",
        "\n",
        "    # Calculate average loss\n",
        "    average_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    return average_loss, accuracy\n",
        "\n",
        "training_set, validation_set, testing_set = get_all_datasets()\n",
        "# XXX make sure val and testing share same coefficients as training set!!\n",
        "\n",
        "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(testing_set, batch_size=batch_size, shuffle=False)\n",
        "# You can write up the rest of the code here. We have already given you most of\n",
        "# what you need to run the module yourself.\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    perform_training_epoch(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    print(\"Validation Results\")\n",
        "    validation_loss, validation_accuracy = perform_inferencing(val_loader)\n",
        "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "# Test the model on the testing set\n",
        "print(\"Testing Results\")\n",
        "test_loss, test_accuracy = perform_inferencing(test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this part of the assignment we use LSTM model to train the data. The F1 score and the accuracy are low because the LSTM predicts the future values based on the past values however in this case we use data from a single breath, which does not use the LSTM's advantage of predicting data."
      ],
      "metadata": {
        "id": "NodUzhSAEfRe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhz15pb5kaA5"
      },
      "source": [
        "## ARDS Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voGXuDFtkaA5"
      },
      "source": [
        "Regardless of whether you were successful on your last assignment, the design was to show you the internal mechanism about how LSTM works.\n",
        "\n",
        "In this assignment you will utilize a dataset of ventilation data taken from 50 subjects. 25 subjects have ARDS, 25 subjects do not have ARDS. Your job is to extract waveform data, and utilize it to perform inferencing on whether the patient has ARDS or not.\n",
        "\n",
        "1. Use basic CNN architecture to perform classification on whether patient has ARDS or not\n",
        "2. Add LSTM to CNN architecture, do results improve? if not why? In this assignment you should use the [PyTorch LSTM layer.](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjheE05kaA6"
      },
      "source": [
        "### Data\n",
        "\n",
        "The data that we use here is ventilation data but it is structured a bit differently than the PVA dataset. Primarily, the data is structured in continuous breath sequences instead of single breaths. Here is an example.\n",
        "\n",
        "<img src=ards-data.png width=50% height=auto\\>\n",
        "\n",
        "This has a few advantages:\n",
        "\n",
        "1. We don't need padding anymore\n",
        "2. It improves performance of our model\n",
        "\n",
        "We stack 20 of these breaths together into a tensor that is in shape `(20, 1, 224)`. This allows us to analyze sequential breaths with an LSTM if we desire."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ventmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtK6ZC_S3SNK",
        "outputId": "a130c986-9fa6-4928-8800-0d6725858df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ventmap\n",
            "  Downloading ventmap-1.5.3.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.5.3)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.0.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ventmap) (3.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ventmap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ventmap) (2023.3.post1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->ventmap) (0.2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ventmap) (1.16.0)\n",
            "Building wheels for collected packages: ventmap\n",
            "  Building wheel for ventmap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ventmap: filename=ventmap-1.5.3-py3-none-any.whl size=40312 sha256=ee0bd851f144d690f2d0cc6301a0edaafe821f5257f4bd34276f80f4482e22fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/18/ac/0abd36110fb734afe3ba7c3e4a69a2c14f8022ee77ba30db13\n",
            "Successfully built ventmap\n",
            "Installing collected packages: ventmap\n",
            "Successfully installed ventmap-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4F1diDpkaA6"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from copy import copy\n",
        "from glob import glob\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import resample\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "from ventmap.raw_utils import read_processed_file\n",
        "\n",
        "from dataset import ARDSDataset\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "def get_dataset(path, name):\n",
        "    saved_set = Path(path)\n",
        "    # Make sure we save previously processed data. This speeds up future processes.\n",
        "    if saved_set.exists():\n",
        "        dataset = ARDSDataset.from_pickle(saved_set.resolve())\n",
        "    else:\n",
        "        dataset = ARDSDataset(224, name, to_pickle=saved_set.resolve())\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_all_datasets():\n",
        "    training_set = get_dataset('ards_training_set.pkl', 'train')\n",
        "    validation_set = get_dataset('ards_validation_set.pkl', 'val')\n",
        "    testing_set = get_dataset('ards_testing_set.pkl', 'test')\n",
        "    return training_set, validation_set, testing_set\n",
        "\n",
        "\n",
        "training_set, validation_set, testing_set = get_all_datasets()\n",
        "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(testing_set, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "from cnn_lstm_net import CNNLSTMNetwork\n",
        "from densenet import densenet18\n",
        "\n",
        "# You are welcome to evaluate other CNN backbones\n",
        "cnn = densenet18()\n",
        "\n",
        "# feel free to modify these parameters\n",
        "lstm_hidden_units = 128\n",
        "lstm_layers = 1\n",
        "\n",
        "# 0 means there is 0% probability of dropout happening\n",
        "lstm_dropout = 0\n",
        "\n",
        "model = CNNLSTMNetwork(cnn, lstm_hidden_units, lstm_layers, lstm_dropout)\n",
        "\n",
        "# We highly recommend using SGD for this problem\n",
        "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
        "bce = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "id": "DP65LdaqhaTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def perform_training_epoch(train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_corrects = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "\n",
        "        *_, y, z= data   # Unpack the inner tuple\n",
        "\n",
        "        x = y.float().cuda()\n",
        "        y = z.float().cuda()\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        model.cuda()\n",
        "        output = model(x)\n",
        "        loss = bce(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        binary_predictions = (output > 0.5).float()\n",
        "        train_corrects += torch.sum(binary_predictions == y.data)\n",
        "        train_total += y.size(0)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = train_corrects / train_total\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def perform_inferencing(loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, y, z in loader:\n",
        "            x = y.float().cuda()\n",
        "\n",
        "            output = model(x)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = bce(output, z.float().cuda())\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "            # Store predictions and ground truth labels\n",
        "            all_predictions.append(output.cpu().numpy())\n",
        "            all_targets.append(z.cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            binary_predictions = (output > 0.5).cpu().float()\n",
        "            total_corrects += torch.sum(binary_predictions == z.data.cpu())\n",
        "            total_samples += z.size(0)\n",
        "\n",
        "    # Concatenate predictions and ground truth labels\n",
        "    predictions = np.concatenate(all_predictions)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Calculate F1 score and accuracy\n",
        "    f1 = f1_score(targets, binary_predictions, average='macro')\n",
        "    accuracy = accuracy_score(targets, binary_predictions)\n",
        "\n",
        "    # Calculate average loss\n",
        "    average_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    print(f\"F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    return average_loss, accuracy\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_accuracy = perform_training_epoch(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    print(\"Validation Results\")\n",
        "    validation_loss, validation_accuracy = perform_inferencing(val_loader)\n",
        "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "# Test the model on the testing set\n",
        "print(\"Testing Results\")\n",
        "test_loss, test_accuracy = perform_inferencing(test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "KtcGGJPvfmor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e63611b-501b-4b01-f068-9fe0e55ef9e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: 0.6896, Training Accuracy: 1.0000\n",
            "Validation Results\n",
            "F1 Score: 0.0000, Accuracy: 0.0000\n",
            "Validation Loss: 0.6910, Validation Accuracy: 0.0000\n",
            "Epoch 2/5\n",
            "Training Loss: 0.6794, Training Accuracy: 0.9999\n",
            "Validation Results\n",
            "F1 Score: 0.0004, Accuracy: 0.0002\n",
            "Validation Loss: 0.6900, Validation Accuracy: 0.0002\n",
            "Epoch 3/5\n",
            "Training Loss: 0.6645, Training Accuracy: 1.0115\n",
            "Validation Results\n",
            "F1 Score: 0.0008, Accuracy: 0.0004\n",
            "Validation Loss: 0.6878, Validation Accuracy: 0.0004\n",
            "Epoch 4/5\n",
            "Training Loss: 0.6313, Training Accuracy: 1.1313\n",
            "Validation Results\n",
            "F1 Score: 0.0008, Accuracy: 0.0004\n",
            "Validation Loss: 0.6870, Validation Accuracy: 0.0004\n",
            "Epoch 5/5\n",
            "Training Loss: 0.5557, Training Accuracy: 1.2869\n",
            "Validation Results\n",
            "F1 Score: 0.3501, Accuracy: 0.2709\n",
            "Validation Loss: 0.6764, Validation Accuracy: 0.2709\n",
            "Testing Results\n",
            "F1 Score: 0.4565, Accuracy: 0.4272\n",
            "Test Accuracy: 0.4272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this part we are suppose to get a higher F1 scores and accuracy because we pass multiple breaths as the input, these help the LTSM remember when a breath is ARDS and when it is not which helps the model in predicting the outputs better, thereby increasing accuracy."
      ],
      "metadata": {
        "id": "vr-esGyNFEGJ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}