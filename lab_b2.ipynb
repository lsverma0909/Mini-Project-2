{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEC 174AY Lab B2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".text_cell_render p{\n",
       "    font-size: 130%;\n",
       "    line-height: 125%;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".text_cell_render p{\n",
    "    font-size: 130%;\n",
    "    line-height: 125%;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will build your skills in utilizing LSTM networks so that you can apply deep learning to time series information\n",
    "\n",
    "1. you will code an LSTM network and apply it to a pre-built codebase. Your focus will be on the ML coding\n",
    "2. You will utilize a partially built code base and then finish it to detect ARDS in waveform data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM is a network that is able to utilize time series information and learn long term patterns to make more accurate predictions than a normal neural network would be able to. We show the general network architecture as an instruction for what you will need to code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_LSTM_cell.png\" width=55% height=auto\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be applying LSTM to the task of patient ventilator asynchrony (PVA) detection. We have supplied a bit of the code you will need. Your jobs will be the following:\n",
    "\n",
    "1. Code the `find_scaling_coefs`, `scale_breath`, and `pad_or_cut_breath` methods in the `PVADataset` class in `dataset.py`.\n",
    "2. Code a simple 1 layer LSTM network based on network schematics given above. You are welcome to use other resource for assistance as well.\n",
    "3. Run your LSTM model on PVA detection. How well does your model perform compared to your original Random Forest classifier? Why are you getting these results?\n",
    "4. Code a more complex 3 layer LSTM network. Do additional layers improve results? Why/Why not?\n",
    "\n",
    "For the math required we would advise you follow the [PyTorch LSTM mathematics](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        # Feel free to modify this\n",
    "        lstm_hidden_units = 32\n",
    "        # First layer is defined for you. Only have 2 input features (flow, pressure)\n",
    "        self.ii = nn.Linear(2, lstm_hidden_units)\n",
    "        # XXX TODO\n",
    "        \n",
    "        # Final layer is defined for you too. Have 3 potential output classes (normal, bsa, dta)\n",
    "        self.final_classification = nn.Linear(lstm_hidden_units, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # XXX code this up\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "you need to code me before things will run",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-10d8ad935a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mval_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-10d8ad935a21>\u001b[0m in \u001b[0;36mget_all_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pva_training_set.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mvalidation_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pva_validation_set.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtesting_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pva_testing_set.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-10d8ad935a21>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(path, name)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# use a sequence length of 224 inputs. If you want to shorten this feel free.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPVADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaved_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/phd/eec193a/ards-lab/dataset.py\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpatient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_scaling_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_scaling_coefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/phd/eec193a/ards-lab/dataset.py\u001b[0m in \u001b[0;36mfind_scaling_coefs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \"\"\"\n\u001b[1;32m    266\u001b[0m         \u001b[0;31m# write function for finding the scaling coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'you need to code me before things will run'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscale_breath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: you need to code me before things will run"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import PVADataset\n",
    "\n",
    "model = LSTMNetwork().cuda()\n",
    "# You should modify the learning rate as suits the problem\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "def get_dataset(path, name):\n",
    "    saved_set = Path(path)\n",
    "    # Make sure we save previously processed data. This speeds up future processes.\n",
    "    if saved_set.exists():\n",
    "        dataset = pd.read_pickle(saved_set.resolve())\n",
    "    else:\n",
    "        # use a sequence length of 224 inputs. If you want to shorten this feel free.\n",
    "        dataset = PVADataset(name, 224)\n",
    "        dataset.process_dataset()\n",
    "        pd.to_pickle(dataset, saved_set.resolve())\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "def get_all_datasets():\n",
    "    training_set = get_dataset('pva_training_set.pkl', 'train')\n",
    "    validation_set = get_dataset('pva_validation_set.pkl', 'val')\n",
    "    testing_set = get_dataset('pva_testing_set.pkl', 'test')\n",
    "    return training_set, validation_set, testing_set\n",
    "\n",
    "\n",
    "\n",
    "def perform_training_epoch(train_loader):\n",
    "    with torch.enable_grad():\n",
    "        for x, y in train_loader:\n",
    "            x = Variable(x.float()).cuda()\n",
    "            y = Variable(y.float()).cuda()\n",
    "            output = model(x)\n",
    "            loss = bce(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Feel free to add diagnostic information like printing loss.\n",
    "            # I will let you handle this.\n",
    "\n",
    "                \n",
    "def perform_inferencing(loader):\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            output = model(x.float()).cuda()\n",
    "            # you must code lines that evaluate performance of the model\n",
    "            # you can use a number of different metrics including loss,\n",
    "            # accuracy, sensitivity, specificity, precision, and f1 score.\n",
    "            #\n",
    "            # For assignment reporting we will require that you use f1 score.\n",
    "            \n",
    "\n",
    "training_set, validation_set, testing_set = get_all_datasets()\n",
    "# XXX make sure val and testing share same coefficients as training set!!\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(testing_set, batch_size=batch_size, shuffle=False)\n",
    "# You can write up the rest of the code here. We have already given you most of\n",
    "# what you need to run the module yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARDS Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of whether you were successful on your last assignment, the design was to show you the internal mechanism about how LSTM works.\n",
    "\n",
    "In this assignment you will utilize a dataset of ventilation data taken from 50 subjects. 25 subjects have ARDS, 25 subjects do not have ARDS. Your job is to extract waveform data, and utilize it to perform inferencing on whether the patient has ARDS or not.\n",
    "\n",
    "1. Use basic CNN architecture to perform classification on whether patient has ARDS or not\n",
    "2. Add LSTM to CNN architecture, do results improve? if not why? In this assignment you should use the [PyTorch LSTM layer.](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data that we use here is ventilation data but it is structured a bit differently than the PVA dataset. Primarily, the data is structured in continuous breath sequences instead of single breaths. Here is an example.\n",
    "\n",
    "<img src=ards-data.png width=50% height=auto\\>\n",
    "\n",
    "This has a few advantages: \n",
    "\n",
    "1. We don't need padding anymore\n",
    "2. It improves performance of our model\n",
    "\n",
    "We stack 20 of these breaths together into a tensor that is in shape `(20, 1, 224)`. This allows us to analyze sequential breaths with an LSTM if we desire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dataset import ARDSDataset\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def get_dataset(path, name):\n",
    "    saved_set = Path(path)\n",
    "    # Make sure we save previously processed data. This speeds up future processes.\n",
    "    if saved_set.exists():\n",
    "        dataset = ARDSDataset.from_pickle(saved_set.resolve())\n",
    "    else:\n",
    "        dataset = ARDSDataset(224, name, to_pickle=saved_set.resolve())\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "def get_all_datasets():\n",
    "    training_set = get_dataset('ards_training_set.pkl', 'train')\n",
    "    validation_set = get_dataset('ards_validation_set.pkl', 'val')\n",
    "    testing_set = get_dataset('ards_testing_set.pkl', 'test')\n",
    "    return training_set, validation_set, testing_set\n",
    "\n",
    "\n",
    "training_set, validation_set, testing_set = get_all_datasets()\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(testing_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "from cnn_lstm_net import CNNLSTMNetwork\n",
    "from densenet import densenet18\n",
    "\n",
    "\n",
    "# You are welcome to evaluate other CNN backbones\n",
    "cnn = densenet18()\n",
    "\n",
    "# feel free to modify these parameters\n",
    "lstm_hidden_units = 128\n",
    "lstm_layers = 1\n",
    "\n",
    "# 0 means there is 0% probability of dropout happening\n",
    "lstm_dropout = 0\n",
    "\n",
    "model = CNNLSTMNetwork(cnn, lstm_hidden_units, lstm_layers, lstm_dropout)\n",
    "\n",
    "# We highly recommend using SGD for this problem\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
    "bce = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
