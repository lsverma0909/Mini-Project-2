{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNSXeaeLkaAr"
      },
      "source": [
        "# EEC 174AY Lab B2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-8NOgK7kiqX",
        "outputId": "32fb66cc-4d0b-476a-e4e8-4128b57cdb4a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "W9zadgdfkaAw",
        "outputId": "fca35b46-d8c9-43ca-a43e-396f4383f3c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              ".text_cell_render p{\n",
              "    font-size: 130%;\n",
              "    line-height: 125%;\n",
              "}\n",
              "</style>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from IPython.core.display import HTML\n",
        "HTML(\"\"\"\n",
        "<style>\n",
        ".text_cell_render p{\n",
        "    font-size: 130%;\n",
        "    line-height: 125%;\n",
        "}\n",
        "</style>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sG0nL9QkaAz"
      },
      "source": [
        "## Outline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGTh8yjCkaAz"
      },
      "source": [
        "This lab will build your skills in utilizing LSTM networks so that you can apply deep learning to time series information\n",
        "\n",
        "1. you will code an LSTM network and apply it to a pre-built codebase. Your focus will be on the ML coding\n",
        "2. You will utilize a partially built code base and then finish it to detect ARDS in waveform data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOuXKvnbkaA0"
      },
      "source": [
        "## LSTM Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkA4ux1ekaA1"
      },
      "source": [
        "LSTM is a network that is able to utilize time series information and learn long term patterns to make more accurate predictions than a normal neural network would be able to. We show the general network architecture as an instruction for what you will need to code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDJqPW_kaA1"
      },
      "source": [
        "# <img src=\"/content/drive/MyDrive/Colab Notebooks/Lab_B2/The_LSTM_cell.png\" width=55% height=auto\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_-8RB9kaA2"
      },
      "source": [
        "You will be applying LSTM to the task of patient ventilator asynchrony (PVA) detection. We have supplied a bit of the code you will need. Your jobs will be the following:\n",
        "\n",
        "1. Code the `find_scaling_coefs`, `scale_breath`, and `pad_or_cut_breath` methods in the `PVADataset` class in `dataset.py`.\n",
        "2. Code a simple 1 layer LSTM network based on network schematics given above. You are welcome to use other resource for assistance as well.\n",
        "3. Run your LSTM model on PVA detection. How well does your model perform compared to your original Random Forest classifier? Why are you getting these results?\n",
        "4. Code a more complex 3 layer LSTM network. Do additional layers improve results? Why/Why not?\n",
        "\n",
        "For the math required we would advise you follow the [PyTorch LSTM mathematics](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "urukombNkaA2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# class LSTMNetwork(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(LSTMNetwork, self).__init__()\n",
        "#         lstm_hidden_units = 32\n",
        "#         self.lstm = nn.LSTM(input_size=2, hidden_size=lstm_hidden_units, batch_first=True)\n",
        "#         self.final_classification = nn.Linear(lstm_hidden_units, 3)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         h0 = torch.zeros(1, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "#         c0 = torch.zeros(1, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "#         out, _ = self.lstm(x, (h0, c0))\n",
        "#         out = self.final_classification(out[:, -1, :])\n",
        "#         return out\n",
        "\n",
        "\n",
        "class LSTMNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        # Feel free to modify this\n",
        "        lstm_hidden_units = 32\n",
        "        # First layer is defined for you. Only have 2 input features (flow, pressure)\n",
        "        self.ii = nn.Linear(2, lstm_hidden_units)\n",
        "        # XXX TODO\n",
        "        self.dummy = nn.Linear(2, lstm_hidden_units)\n",
        "        self.s2 = nn.Linear(2, lstm_hidden_units)\n",
        "        self.s3 = nn.Linear(2, lstm_hidden_units)\n",
        "        self.t1 = nn.Linear(2, lstm_hidden_units)\n",
        "        # Final layer is defined for you too. Have 3 potential output classes (normal, bsa, dta)\n",
        "        self.final_classification = nn.Linear(lstm_hidden_units, 3)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # XXX code this up\n",
        "        trial = self.dummy(x[:, 1, :])\n",
        "        trial = self.sigmoid(trial)\n",
        "        ct_next = torch.zeros_like(trial)\n",
        "        for i in range(x.size(1)):\n",
        "          r = x[:, i, :]\n",
        "          ct_prev = ct_next\n",
        "          sig1 = self.ii(r)\n",
        "          sig1 = self.sigmoid(sig1)\n",
        "          sig2 = self.s2(r)\n",
        "          sig2 = self.sigmoid(sig2)\n",
        "          sig3 = self.s3(r)\n",
        "          sig3 = self.sigmoid(sig3)\n",
        "          tan1 = self.t1(r)\n",
        "          tan1 = self.tanh(tan1)\n",
        "          ct_s1 = ct_prev * sig1\n",
        "          s2_t1 = sig2 * tan1\n",
        "          ct_next = ct_s1 + s2_t1\n",
        "          ct_tan = self.tanh(ct_next)\n",
        "          out = sig3 * ct_tan\n",
        "          out = self.final_classification(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ix_uyXtbkaA3",
        "outputId": "92aa11b2-9d17-4991-e876-e0342427a8ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: 0.5999, Training Accuracy: 2.0000\n",
            "Validation Results\n",
            "F1 Score: 0.0020\n",
            "Validation Loss: 0.5794, Validation Accuracy: 2.0003\n",
            "Epoch 2/5\n",
            "Training Loss: 0.5254, Training Accuracy: 2.1912\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5620, Validation Accuracy: 2.1400\n",
            "Epoch 3/5\n",
            "Training Loss: 0.5076, Training Accuracy: 2.3090\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5603, Validation Accuracy: 2.1400\n",
            "Epoch 4/5\n",
            "Training Loss: 0.5019, Training Accuracy: 2.3090\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5609, Validation Accuracy: 2.1400\n",
            "Epoch 5/5\n",
            "Training Loss: 0.4998, Training Accuracy: 2.3090\n",
            "Validation Results\n",
            "F1 Score: 0.5700\n",
            "Validation Loss: 0.5619, Validation Accuracy: 2.1400\n",
            "Testing Results\n",
            "F1 Score: 0.4960\n",
            "Test Accuracy: 1.9920\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class PVADataset(Dataset):\n",
        "    def __init__(self, dataset_type, sequence_len):\n",
        "        \"\"\"\n",
        "        Extract breath data and corresponding PVA annotations\n",
        "\n",
        "        :param dataset_type: What set we are using (train/val/test)\n",
        "        :param sequence_len: The length of the sequences we want to give to our LSTM\n",
        "        \"\"\"\n",
        "        if dataset_type not in ['train', 'val', 'test']:\n",
        "            raise Exception('dataset_type must be either \"train\", \"val\", or \"test\"')\n",
        "\n",
        "        dataset_path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/Lab_B2', 'pva_dataset', dataset_type)\n",
        "        self.record_set = glob(os.path.join(dataset_path, '*_data.pkl'))\n",
        "        self.all_sequences = []\n",
        "        self.sequence_len = sequence_len\n",
        "\n",
        "    def process_dataset(self):\n",
        "        \"\"\"\n",
        "        Extract all breaths in the dataset and pair a ground truth value\n",
        "        with the breath information\n",
        "        \"\"\"\n",
        "        for record in self.record_set:\n",
        "            data = pd.read_pickle(record)\n",
        "            gt = pd.read_pickle(record.replace('_data.pkl', '_gt.pkl'))\n",
        "            patient = os.path.basename(record).split('_')[0]\n",
        "            for i, b in enumerate(data):\n",
        "                gt_row = gt.iloc[i]\n",
        "                if gt_row.bn != b['rel_bn']:\n",
        "                    raise Exception('something went wrong with gt parsing for record {}'.format(record))\n",
        "                if gt_row.dta >= 1:\n",
        "                    y = [0, 0, 1]\n",
        "                elif gt_row.bsa >= 1:\n",
        "                    y = [0, 1, 0]\n",
        "                else:\n",
        "                    y = [1, 0, 0]\n",
        "\n",
        "                tensor = np.array([b['flow'], b['pressure']]).transpose()\n",
        "                self.all_sequences.append([patient, tensor, np.array(y)])\n",
        "\n",
        "        self.find_scaling_coefs()\n",
        "\n",
        "    def find_scaling_coefs(self):\n",
        "        \"\"\"\n",
        "        In order to conduct scaling you will need to find some scaling\n",
        "        coefficients that are represented in our data. The time to find\n",
        "        these coefficients is right after the data has been processed\n",
        "        into a machine-usable format\n",
        "        \"\"\"\n",
        "        # write function for finding the scaling coefficients\n",
        "        all_data = np.concatenate([item[1] for item in self.all_sequences], axis=0)\n",
        "        # pressure_and_flow_values = data_array[:, :2]  # Extracting the first two columns (pressure and flow)\n",
        "\n",
        "        # Calculate mean and standard deviation for pressure and flow\n",
        "        # mean_values = np.mean(all_data, axis=0)\n",
        "        # std_values = np.std(all_data, axis=0)\n",
        "\n",
        "        # print(\"Mean Values (Pressure, Flow):\", mean_values)\n",
        "        # print(\"Standard Deviation Values (Pressure, Flow):\", std_values)\n",
        "        # print(self.all_sequences[:2])\n",
        "        # print(all_data[:5])\n",
        "        self.scale_breath(all_data)\n",
        "\n",
        "    def scale_breath(self, data):\n",
        "        \"\"\"\n",
        "        Scale breath using any number of scaling techniques learned in\n",
        "        this class. You can use standardization, max-min scaling, or\n",
        "        anything else that you'd like to code\n",
        "        \"\"\"\n",
        "        # mean = self.scaling_coefficients['mean']\n",
        "        # print(mean.shape())\n",
        "        # std  = self.scaling_coefficients['std']\n",
        "        # print(std.shape())\n",
        "        mean_values = np.mean(data, axis=0)\n",
        "        std_values = np.std(data, axis=0)\n",
        "        output = (data - mean_values) / std_values\n",
        "        # print(data)\n",
        "        # print(output)\n",
        "        return output\n",
        "\n",
        "    def pad_or_cut_breath(self, data):\n",
        "        \"\"\"\n",
        "        For purposes of the simple LSTM that you are going to code you\n",
        "        will need to have all your breaths be of uniform size. This means\n",
        "        adding a padded value like 0 to a sequence to ensure a breath reaches\n",
        "        desired length. It could also mean removing observations from a\n",
        "        sequence if the data is longer than desired length\n",
        "        \"\"\"\n",
        "        desired_length = self.sequence_len\n",
        "        current_length = len(data)\n",
        "\n",
        "        if current_length < desired_length:\n",
        "            # Pad with zeros to reach the desired length\n",
        "            padding = np.zeros((desired_length - current_length, data.shape[1]))\n",
        "            data = np.concatenate([data, padding], axis=0)\n",
        "        elif current_length > desired_length:\n",
        "            # Truncate to the desired length\n",
        "            data = data[:desired_length, :]\n",
        "            # print(data.shape())\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        get next sequence\n",
        "        \"\"\"\n",
        "        pt, data, y = self.all_sequences[idx]\n",
        "        data = self.scale_breath(data)\n",
        "        data = self.pad_or_cut_breath(data)\n",
        "        return data, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_sequences)\n",
        "\n",
        "model = LSTMNetwork().cuda()\n",
        "# You should modify the learning rate as suits the problem\n",
        "optimizer = SGD(model.parameters(), lr=0.01)\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "def get_dataset(path, name):\n",
        "    saved_set = Path(path)\n",
        "    # Make sure we save previously processed data. This speeds up future processes.\n",
        "    if saved_set.exists():\n",
        "        dataset = pd.read_pickle(saved_set.resolve())\n",
        "    else:\n",
        "        # use a sequence length of 224 inputs. If you want to shorten this feel free.\n",
        "        dataset = PVADataset(name, 224)\n",
        "        dataset.process_dataset()\n",
        "        pd.to_pickle(dataset, saved_set.resolve())\n",
        "    return dataset\n",
        "\n",
        "def get_all_datasets():\n",
        "    training_set = get_dataset('pva_training_set.pkl', 'train')\n",
        "    validation_set = get_dataset('pva_validation_set.pkl', 'val')\n",
        "    testing_set = get_dataset('pva_testing_set.pkl', 'test')\n",
        "    return training_set, validation_set, testing_set\n",
        "\n",
        "def perform_training_epoch(train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_corrects = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for x, y in train_loader:\n",
        "        x = x.float().cuda()\n",
        "        y = y.float().cuda()\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        output = model(x)\n",
        "        loss = bce(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        binary_predictions = (output > 0.5).float()\n",
        "        train_corrects += torch.sum(binary_predictions == y.data)\n",
        "        train_total += y.size(0)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = train_corrects / train_total\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "def perform_inferencing(loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.float().cuda()\n",
        "\n",
        "            output = model(x)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = bce(output, y.float().cuda())\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "            # Store predictions and ground truth labels\n",
        "            all_predictions.append(output.cpu().numpy())\n",
        "            all_targets.append(y.cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            binary_predictions = (output > 0.5).cpu().float()  # Move to CPU\n",
        "            total_corrects += torch.sum(binary_predictions == y.data.cpu())\n",
        "            total_samples += y.size(0)\n",
        "\n",
        "    # Concatenate predictions and ground truth labels\n",
        "    predictions = np.concatenate(all_predictions)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Calculate F1 score\n",
        "    f1 = f1_score(targets, binary_predictions, average='micro')\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = total_corrects / total_samples\n",
        "\n",
        "    # Calculate average loss\n",
        "    average_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    return average_loss, accuracy\n",
        "\n",
        "training_set, validation_set, testing_set = get_all_datasets()\n",
        "# XXX make sure val and testing share same coefficients as training set!!\n",
        "\n",
        "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(testing_set, batch_size=batch_size, shuffle=False)\n",
        "# You can write up the rest of the code here. We have already given you most of\n",
        "# what you need to run the module yourself.\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    perform_training_epoch(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    print(\"Validation Results\")\n",
        "    validation_loss, validation_accuracy = perform_inferencing(val_loader)\n",
        "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "# Test the model on the testing set\n",
        "print(\"Testing Results\")\n",
        "test_loss, test_accuracy = perform_inferencing(test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhz15pb5kaA5"
      },
      "source": [
        "## ARDS Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voGXuDFtkaA5"
      },
      "source": [
        "Regardless of whether you were successful on your last assignment, the design was to show you the internal mechanism about how LSTM works.\n",
        "\n",
        "In this assignment you will utilize a dataset of ventilation data taken from 50 subjects. 25 subjects have ARDS, 25 subjects do not have ARDS. Your job is to extract waveform data, and utilize it to perform inferencing on whether the patient has ARDS or not.\n",
        "\n",
        "1. Use basic CNN architecture to perform classification on whether patient has ARDS or not\n",
        "2. Add LSTM to CNN architecture, do results improve? if not why? In this assignment you should use the [PyTorch LSTM layer.](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjheE05kaA6"
      },
      "source": [
        "### Data\n",
        "\n",
        "The data that we use here is ventilation data but it is structured a bit differently than the PVA dataset. Primarily, the data is structured in continuous breath sequences instead of single breaths. Here is an example.\n",
        "\n",
        "<img src=ards-data.png width=50% height=auto\\>\n",
        "\n",
        "This has a few advantages:\n",
        "\n",
        "1. We don't need padding anymore\n",
        "2. It improves performance of our model\n",
        "\n",
        "We stack 20 of these breaths together into a tensor that is in shape `(20, 1, 224)`. This allows us to analyze sequential breaths with an LSTM if we desire."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ventmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtK6ZC_S3SNK",
        "outputId": "a130c986-9fa6-4928-8800-0d6725858df6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ventmap\n",
            "  Downloading ventmap-1.5.3.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.23.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.5.3)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.0.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from ventmap) (3.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ventmap) (1.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ventmap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ventmap) (2023.3.post1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->ventmap) (0.2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ventmap) (1.16.0)\n",
            "Building wheels for collected packages: ventmap\n",
            "  Building wheel for ventmap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ventmap: filename=ventmap-1.5.3-py3-none-any.whl size=40312 sha256=ee0bd851f144d690f2d0cc6301a0edaafe821f5257f4bd34276f80f4482e22fd\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/18/ac/0abd36110fb734afe3ba7c3e4a69a2c14f8022ee77ba30db13\n",
            "Successfully built ventmap\n",
            "Installing collected packages: ventmap\n",
            "Successfully installed ventmap-1.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "S4F1diDpkaA6"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from copy import copy\n",
        "from glob import glob\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.signal import resample\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "from ventmap.raw_utils import read_processed_file\n",
        "\n",
        "class ARDSDataset(Dataset):\n",
        "    def __init__(self, seq_len, dataset_type, to_pickle=None):\n",
        "        \"\"\"\n",
        "        Dataset to generate sequences of data for ARDS Detection\n",
        "        \"\"\"\n",
        "        data_path = os.path.join('/content/drive/MyDrive/Colab_Notebooks/EEC174LabB2', 'data')\n",
        "        cohort_file = os.path.join(data_path, 'anon-desc.csv')\n",
        "        self.seq_len = seq_len\n",
        "        self.n_sub_batches = 20\n",
        "        self.all_sequences = []\n",
        "\n",
        "        self.cohort = pd.read_csv(cohort_file)\n",
        "        self.cohort = self.cohort.rename(columns={'Patient Unique Identifier': 'patient_id'})\n",
        "        self.cohort['patient_id'] = self.cohort['patient_id'].astype(str)\n",
        "\n",
        "        raw_dir = os.path.join(data_path, 'experiment1', dataset_type, 'raw')\n",
        "        if not os.path.exists(raw_dir):\n",
        "            raise Exception('No directory {} exists!'.format(raw_dir))\n",
        "        self.raw_files = sorted(glob(os.path.join(raw_dir, '*/*.raw.npy')))\n",
        "        self.processed_files = sorted(glob(os.path.join(raw_dir, '*/*.processed.npy')))\n",
        "        self.get_dataset()\n",
        "        self.derive_scaling_factors()\n",
        "        if to_pickle:\n",
        "            pd.to_pickle(self, to_pickle)\n",
        "\n",
        "    def derive_scaling_factors(self):\n",
        "        indices = [range(len(self.all_sequences))]\n",
        "        self.scaling_factors = {\n",
        "            None: self._get_scaling_factors_for_indices(idxs)\n",
        "            for i, idxs in enumerate(indices)\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_pickle(self, data_path):\n",
        "        dataset = pd.read_pickle(data_path)\n",
        "        if not isinstance(dataset, ARDSDataset):\n",
        "            raise ValueError('The pickle file you have specified is out-of-date. Please re-process your dataset and save the new pickled dataset.')\n",
        "        # paranoia\n",
        "        try:\n",
        "            dataset.scaling_factors\n",
        "        except AttributeError:\n",
        "            dataset.derive_scaling_factors()\n",
        "        return dataset\n",
        "\n",
        "    def get_dataset(self):\n",
        "        last_patient = None\n",
        "        for fidx, filename in enumerate(self.raw_files):\n",
        "            gen = read_processed_file(filename, filename.replace('.raw.npy', '.processed.npy'))\n",
        "            patient_id = self._get_patient_id_from_file(filename)\n",
        "\n",
        "            if patient_id != last_patient:\n",
        "                batch_arr = []\n",
        "                breath_arr = []\n",
        "                seq_vent_bns = []\n",
        "                batch_seq_hours = []\n",
        "\n",
        "            last_patient = patient_id\n",
        "            target = self._pathophysiology_target(patient_id)\n",
        "            start_time = self._get_patient_start_time(patient_id)\n",
        "\n",
        "            for bidx, breath in enumerate(gen):\n",
        "                # cutoff breaths if they have too few points.\n",
        "                if len(breath['flow']) < 21:\n",
        "                    continue\n",
        "\n",
        "                breath_time = self.get_abs_bs_dt(breath)\n",
        "                if breath_time < start_time:\n",
        "                    continue\n",
        "                elif breath_time > start_time + pd.Timedelta(hours=24):\n",
        "                    break\n",
        "\n",
        "                flow = breath['flow']\n",
        "                seq_hour = (breath_time - start_time).total_seconds() / 60 / 60\n",
        "                seq_vent_bns.append(breath['vent_bn'])\n",
        "                batch_arr, breath_arr, batch_seq_hours = self._unpadded_centered_processing(\n",
        "                    flow, breath_arr, batch_arr, batch_seq_hours, seq_hour\n",
        "                )\n",
        "\n",
        "                if len(batch_arr) == self.n_sub_batches:\n",
        "                    raw_data = np.array(batch_arr)\n",
        "                    breath_window = raw_data.reshape((self.n_sub_batches, 1, self.seq_len))\n",
        "                    self.all_sequences.append([patient_id, breath_window, target, batch_seq_hours])\n",
        "                    batch_arr = []\n",
        "                    seq_vent_bns = []\n",
        "                    batch_seq_hours = []\n",
        "\n",
        "                if len(batch_arr) > 0 and breath_arr == []:\n",
        "                    batch_seq_hours.append(seq_hour)\n",
        "\n",
        "    def get_abs_bs_dt(self, breath):\n",
        "        if isinstance(breath['abs_bs'], bytes):\n",
        "            breath['abs_bs'] = breath['abs_bs'].decode('utf-8')\n",
        "        try:\n",
        "            breath_time = pd.to_datetime(breath['abs_bs'], format='%Y-%m-%d %H-%M-%S.%f')\n",
        "        except:\n",
        "            breath_time = pd.to_datetime(breath['abs_bs'], format='%Y-%m-%d %H:%M:%S.%f')\n",
        "        return breath_time\n",
        "\n",
        "    def _pathophysiology_target(self, patient_id):\n",
        "        patient_row = self.cohort[self.cohort['patient_id'] == patient_id]\n",
        "        try:\n",
        "            patient_row = patient_row.iloc[0]\n",
        "        except:\n",
        "            raise ValueError('Could not find patient {} in cohort file'.format(patient_id))\n",
        "        patho = 1 if patient_row['Pathophysiology'] == 'ARDS' else 0\n",
        "        target = np.zeros(2)\n",
        "        target[patho] = 1\n",
        "        return target\n",
        "\n",
        "    def _unpadded_centered_processing(self, flow, breath_arr, batch_arr, batch_seq_hours, seq_hour):\n",
        "        if (len(flow) + len(breath_arr)) < self.seq_len:\n",
        "            breath_arr.extend(flow)\n",
        "        else:\n",
        "            remaining = self.seq_len - len(breath_arr)\n",
        "            breath_arr.extend(flow[:remaining])\n",
        "            batch_arr.append(np.array(breath_arr))\n",
        "            batch_seq_hours.append(seq_hour)\n",
        "            breath_arr = []\n",
        "        return batch_arr, breath_arr, batch_seq_hours\n",
        "\n",
        "    def _get_scaling_factors_for_indices(self, indices):\n",
        "        \"\"\"\n",
        "        Get mu and std for a specific set of indices\n",
        "        \"\"\"\n",
        "        std_sum = 0\n",
        "        mean_sum = 0\n",
        "        obs_count = 0\n",
        "\n",
        "        for idx in indices:\n",
        "            obs = self.all_sequences[idx][1]\n",
        "            obs_count += len(obs)\n",
        "            mean_sum += obs.sum()\n",
        "        mu = mean_sum / obs_count\n",
        "\n",
        "        # calculate std\n",
        "        for idx in indices:\n",
        "            obs = self.all_sequences[idx][1]\n",
        "            std_sum += ((obs - mu) ** 2).sum()\n",
        "        std = np.sqrt(std_sum / obs_count)\n",
        "        return mu, std\n",
        "\n",
        "    def _pathophysiology_target(self, patient_id):\n",
        "        patient_row = self.cohort[self.cohort['patient_id'] == patient_id]\n",
        "        try:\n",
        "            patient_row = patient_row.iloc[0]\n",
        "        except:\n",
        "            raise ValueError('Could not find patient {} in cohort file'.format(patient_id))\n",
        "        patho = 1 if patient_row['Pathophysiology'] == 'ARDS' else 0\n",
        "        target = np.zeros(2)\n",
        "        target[patho] = 1\n",
        "        return target\n",
        "\n",
        "    def _get_patient_start_time(self, patient_id):\n",
        "        patient_row = self.cohort[self.cohort['patient_id'] == patient_id]\n",
        "        patient_row = patient_row.iloc[0]\n",
        "        patho = 1 if patient_row['Pathophysiology'] == 'ARDS' else 0\n",
        "        if patho == 1:\n",
        "            start_time = pd.to_datetime(patient_row['Date when Berlin criteria first met (m/dd/yyy)'])\n",
        "        else:\n",
        "            start_time = pd.to_datetime(patient_row['vent_start_time'])\n",
        "\n",
        "        if start_time is pd.NaT:\n",
        "            raise Exception('Could not find valid start time for {}'.format(patient_id))\n",
        "        return start_time\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        seq = self.all_sequences[index]\n",
        "        pt, data, target, _ = seq\n",
        "        try:\n",
        "            mu, std = self.scaling_factors[None]\n",
        "        except AttributeError:\n",
        "            raise AttributeError('Scaling factors not found for dataset. You must derive them using the `derive_scaling_factors` function.')\n",
        "        data = (data - mu) / std\n",
        "\n",
        "        return pt, data, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_sequences)\n",
        "\n",
        "    def get_ground_truth_df(self):\n",
        "        return self._get_all_sequence_ground_truth()\n",
        "\n",
        "    def _get_all_sequence_ground_truth(self):\n",
        "        rows = []\n",
        "        for seq in self.all_sequences:\n",
        "            patient, _, target = seq\n",
        "            rows.append([patient, np.argmax(target, axis=0)])\n",
        "        return pd.DataFrame(rows, columns=['patient', 'y'])\n",
        "\n",
        "    def _get_patient_id_from_file(self, filename):\n",
        "        pt_id = filename.split('/')[-2]\n",
        "        # sanity check to see if patient\n",
        "        match = re.search(r'(0\\d{3}RPI\\d{10})', filename)\n",
        "        if match:\n",
        "            return match.groups()[0]\n",
        "        try:\n",
        "            # id is from anonymous dataset\n",
        "            float(pt_id)\n",
        "            return pt_id\n",
        "        except:\n",
        "            raise ValueError('could not find patient id in file: {}'.format(filename))\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "def get_dataset(path, name):\n",
        "    saved_set = Path(path)\n",
        "    # Make sure we save previously processed data. This speeds up future processes.\n",
        "    if saved_set.exists():\n",
        "        dataset = ARDSDataset.from_pickle(saved_set.resolve())\n",
        "    else:\n",
        "        dataset = ARDSDataset(224, name, to_pickle=saved_set.resolve())\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_all_datasets():\n",
        "    training_set = get_dataset('ards_training_set.pkl', 'train')\n",
        "    validation_set = get_dataset('ards_validation_set.pkl', 'val')\n",
        "    testing_set = get_dataset('ards_testing_set.pkl', 'test')\n",
        "    return training_set, validation_set, testing_set\n",
        "\n",
        "\n",
        "training_set, validation_set, testing_set = get_all_datasets()\n",
        "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(testing_set, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "class CNNLSTMNetwork(nn.Module):\n",
        "    def __init__(self, cnn_network, lstm_hidden_units, lstm_layers, lstm_dropout):\n",
        "        super(CNNLSTMNetwork, self).__init__()\n",
        "\n",
        "        self.cnn = cnn_network\n",
        "        self.lstm_hidden_units = lstm_hidden_units\n",
        "        # Add LSTM layer\n",
        "        self.lstm = nn.LSTM(input_size=lstm_hidden_units,\n",
        "                            hidden_size=lstm_hidden_units,\n",
        "                            num_layers=lstm_layers,\n",
        "                            dropout=lstm_dropout,\n",
        "                            batch_first=True)\n",
        "\n",
        "        # Add linear layer\n",
        "        self.fc = nn.Linear(lstm_hidden_units, 2)\n",
        "\n",
        "    def forward(self, x, hx_cx=None):\n",
        "        # input should be in shape: (batches, breaths in seq, chans, 224)\n",
        "        batches, seq_len, chans, _ = x.shape\n",
        "\n",
        "        # Initialize an empty list to store outputs\n",
        "        outputs = []\n",
        "\n",
        "        # Process the first input breath separately\n",
        "        cnn_out = self.cnn(x[:, 0, 0, :].unsqueeze(1)).squeeze()\n",
        "        lstm_out, _ = self.lstm(cnn_out.unsqueeze(1), hx_cx)\n",
        "        lstm_out = lstm_out[:, -1, :]\n",
        "        output = self.fc(lstm_out)\n",
        "        outputs.append(output)\n",
        "\n",
        "        # Process the remaining input breaths\n",
        "        intermediate_outputs = []  # List to store intermediate tensors\n",
        "        for i in range(1, seq_len):\n",
        "            cnn_out = self.cnn(x[:, i, 0, :].unsqueeze(1)).squeeze()\n",
        "            lstm_out, _ = self.lstm(cnn_out.unsqueeze(1))\n",
        "            lstm_out = lstm_out[:, -1, :]\n",
        "            output = self.fc(lstm_out)\n",
        "            intermediate_outputs.append(output)\n",
        "\n",
        "        # Concatenate the intermediate tensors along the sequence dimension\n",
        "        outputs = torch.cat([outputs[0]] + intermediate_outputs, dim=0)\n",
        "\n",
        "        return output\n",
        "    # def forward(self, x, hx_cx=None):\n",
        "    #     # input should be in shape: (batches, breaths in seq, chans, 224)\n",
        "    #     batches, seq_len, chans, _ = x.shape\n",
        "\n",
        "    #     # Initialize an empty list to store intermediate LSTM outputs\n",
        "    #     intermediate_outputs = []\n",
        "\n",
        "    #     # Process each input breath\n",
        "    #     for i in range(seq_len):\n",
        "    #         cnn_out = self.cnn(x[:, i, 0, :].unsqueeze(1)).squeeze()\n",
        "\n",
        "    #         # Use hidden and cell states for the LSTM layer\n",
        "    #         lstm_out, (hidden_state, cell_state) = self.lstm(cnn_out.unsqueeze(1), hx_cx)\n",
        "    #         lstm_out = lstm_out[:, -1, :]\n",
        "\n",
        "    #         # Store intermediate LSTM outputs\n",
        "    #         intermediate_outputs.append(lstm_out)\n",
        "\n",
        "    #     # Concatenate the intermediate LSTM outputs along the sequence dimension\n",
        "    #     intermediate_outputs = torch.cat(intermediate_outputs, dim=0)\n",
        "\n",
        "    #     # Apply the fully connected layer to the concatenated intermediate LSTM outputs\n",
        "    #     output = self.fc(intermediate_outputs)\n",
        "\n",
        "    #     return output\n",
        "\n",
        "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\n",
        "\n",
        "model_urls = {\n",
        "    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n",
        "    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n",
        "    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n",
        "    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n",
        "}\n",
        "\n",
        "\n",
        "class _DenseLayer(nn.Sequential):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm1d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv1d(num_input_features, bn_size *\n",
        "                                           growth_rate, kernel_size=1, stride=1,\n",
        "                                           bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm1d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv1d(bn_size * growth_rate, growth_rate,\n",
        "                                           kernel_size=3, stride=1, padding=1,\n",
        "                                           bias=False)),\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
        "                                     training=self.training)\n",
        "        return torch.cat([x, new_features], 1)\n",
        "\n",
        "\n",
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate,\n",
        "                                bn_size, drop_rate)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)\n",
        "\n",
        "\n",
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm1d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv1d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        self.add_module('pool', nn.AvgPool1d(kernel_size=2, stride=2))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    r\"\"\"Densenet-BC model class, based on\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        growth_rate (int) - how many filters to add each layer (`k` in paper)\n",
        "        block_config (list of 4 ints) - how many layers in each pooling block\n",
        "        num_init_features (int) - the number of filters to learn in the first convolution layer\n",
        "        bn_size (int) - multiplicative factor for number of bottle neck layers\n",
        "          (i.e. bn_size * k features in the bottleneck layer)\n",
        "        drop_rate (float) - dropout rate after each dense layer\n",
        "        num_classes (int) - number of classification classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0.2, num_classes=1000):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.inplanes = num_init_features\n",
        "\n",
        "        # First convolution\n",
        "        self.features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv1d(1, num_init_features, kernel_size=7, stride=2,\n",
        "                                padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm1d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool1d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate,\n",
        "                                drop_rate=drop_rate)\n",
        "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                trans = _Transition(num_input_features=num_features,\n",
        "                                    num_output_features=num_features // 2)\n",
        "                self.features.add_module('transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "        # Final batch norm\n",
        "        self.features.add_module('norm5', nn.BatchNorm1d(num_features))\n",
        "\n",
        "        # Linear layer\n",
        "        #self.classifier = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # Official init from torch repo.\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                n = m.kernel_size[0] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        self.n_out_filters = num_features\n",
        "        self.avgpool = nn.AvgPool1d(7, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.features(x)\n",
        "        #print(features.size())\n",
        "        out = F.relu(features, inplace=True)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(features.size(0), -1)\n",
        "        #out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n",
        "        #out = self.classifier(out)\n",
        "        #print(out.size())\n",
        "        #print(self.n_out_filters)\n",
        "        return out\n",
        "\n",
        "\n",
        "def _load_state_dict(model, model_url, progress):\n",
        "    # '.'s are no longer allowed in module names, but previous _DenseLayer\n",
        "    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n",
        "    # They are also in the checkpoints in model_urls. This pattern is used\n",
        "    # to find such keys.\n",
        "    pattern = re.compile(\n",
        "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
        "\n",
        "    state_dict = load_state_dict_from_url(model_url, progress=progress)\n",
        "    for key in list(state_dict.keys()):\n",
        "        res = pattern.match(key)\n",
        "        if res:\n",
        "            new_key = res.group(1) + res.group(2)\n",
        "            state_dict[new_key] = state_dict[key]\n",
        "            del state_dict[key]\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n",
        "              **kwargs):\n",
        "    model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n",
        "    model.network_name = arch\n",
        "    if pretrained:\n",
        "        _load_state_dict(model, model_urls[arch], progress)\n",
        "    return model\n",
        "\n",
        "\n",
        "def densenet18(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-18 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _densenet('densenet18', 32, (2, 2, 2, 2), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "def densenet121(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-121 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _densenet('densenet121', 32, (6, 12, 24, 16), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "def densenet161(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-161 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _densenet('densenet161', 48, (6, 12, 36, 24), 96, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "def densenet169(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-169 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _densenet('densenet169', 32, (6, 12, 32, 32), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "def densenet201(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"Densenet-201 model from\n",
        "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _densenet('densenet201', 32, (6, 12, 48, 32), 64, pretrained, progress,\n",
        "                     **kwargs)\n",
        "\n",
        "\n",
        "# You are welcome to evaluate other CNN backbones\n",
        "cnn = densenet18()\n",
        "\n",
        "# feel free to modify these parameters\n",
        "lstm_hidden_units = 128\n",
        "lstm_layers = 1\n",
        "\n",
        "# 0 means there is 0% probability of dropout happening\n",
        "lstm_dropout = 0\n",
        "\n",
        "model = CNNLSTMNetwork(cnn, lstm_hidden_units, lstm_layers, lstm_dropout)\n",
        "\n",
        "# We highly recommend using SGD for this problem\n",
        "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0001)\n",
        "bce = nn.BCEWithLogitsLoss()\n"
      ],
      "metadata": {
        "id": "DP65LdaqhaTA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def perform_training_epoch(train_loader):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_corrects = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for data in train_loader:\n",
        "\n",
        "        *_, y, z= data   # Unpack the inner tuple\n",
        "\n",
        "        x = y.float().cuda()\n",
        "        y = z.float().cuda()\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        model.cuda()\n",
        "        output = model(x)\n",
        "        loss = bce(output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update training loss\n",
        "        train_loss += loss.item() * x.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        binary_predictions = (output > 0.5).float()\n",
        "        train_corrects += torch.sum(binary_predictions == y.data)\n",
        "        train_total += y.size(0)\n",
        "\n",
        "    # Calculate training accuracy\n",
        "    train_accuracy = train_corrects / train_total\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    print(f\"Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    return train_loss, train_accuracy\n",
        "\n",
        "\n",
        "def perform_inferencing(loader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    total_loss = 0.0\n",
        "    total_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _, y, z in loader:\n",
        "            x = y.float().cuda()\n",
        "\n",
        "            output = model(x)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = bce(output, z.float().cuda())\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "\n",
        "            # Store predictions and ground truth labels\n",
        "            all_predictions.append(output.cpu().numpy())\n",
        "            all_targets.append(z.cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy\n",
        "            binary_predictions = (output > 0.5).cpu().float()\n",
        "            total_corrects += torch.sum(binary_predictions == z.data.cpu())\n",
        "            total_samples += z.size(0)\n",
        "\n",
        "    # Concatenate predictions and ground truth labels\n",
        "    predictions = np.concatenate(all_predictions)\n",
        "    targets = np.concatenate(all_targets)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    binary_predictions = (predictions > 0.5).astype(int)\n",
        "\n",
        "    # Calculate F1 score and accuracy\n",
        "    f1 = f1_score(targets, binary_predictions, average='macro')\n",
        "    accuracy = accuracy_score(targets, binary_predictions)\n",
        "\n",
        "    # Calculate average loss\n",
        "    average_loss = total_loss / len(loader.dataset)\n",
        "\n",
        "    print(f\"F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "    return average_loss, accuracy\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "    # Training\n",
        "    train_loss, train_accuracy = perform_training_epoch(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    print(\"Validation Results\")\n",
        "    validation_loss, validation_accuracy = perform_inferencing(val_loader)\n",
        "    print(f\"Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
        "\n",
        "# Test the model on the testing set\n",
        "print(\"Testing Results\")\n",
        "test_loss, test_accuracy = perform_inferencing(test_loader)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "KtcGGJPvfmor",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e63611b-501b-4b01-f068-9fe0e55ef9e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "Training Loss: 0.6896, Training Accuracy: 1.0000\n",
            "Validation Results\n",
            "F1 Score: 0.0000, Accuracy: 0.0000\n",
            "Validation Loss: 0.6910, Validation Accuracy: 0.0000\n",
            "Epoch 2/5\n",
            "Training Loss: 0.6794, Training Accuracy: 0.9999\n",
            "Validation Results\n",
            "F1 Score: 0.0004, Accuracy: 0.0002\n",
            "Validation Loss: 0.6900, Validation Accuracy: 0.0002\n",
            "Epoch 3/5\n",
            "Training Loss: 0.6645, Training Accuracy: 1.0115\n",
            "Validation Results\n",
            "F1 Score: 0.0008, Accuracy: 0.0004\n",
            "Validation Loss: 0.6878, Validation Accuracy: 0.0004\n",
            "Epoch 4/5\n",
            "Training Loss: 0.6313, Training Accuracy: 1.1313\n",
            "Validation Results\n",
            "F1 Score: 0.0008, Accuracy: 0.0004\n",
            "Validation Loss: 0.6870, Validation Accuracy: 0.0004\n",
            "Epoch 5/5\n",
            "Training Loss: 0.5557, Training Accuracy: 1.2869\n",
            "Validation Results\n",
            "F1 Score: 0.3501, Accuracy: 0.2709\n",
            "Validation Loss: 0.6764, Validation Accuracy: 0.2709\n",
            "Testing Results\n",
            "F1 Score: 0.4565, Accuracy: 0.4272\n",
            "Test Accuracy: 0.4272\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}